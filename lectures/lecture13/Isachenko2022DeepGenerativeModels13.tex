\input{../utils/preamble}
\createdgmtitle{13}

\usepackage{tikz}

\usetikzlibrary{arrows,shapes,positioning,shadows,trees}
%--------------------------------------------------------------------------------
\begin{document}
%--------------------------------------------------------------------------------
\begin{frame}[noframenumbering,plain]
%\thispagestyle{empty}
\titlepage
\end{frame}
%=======
\begin{frame}{Outline}
	\tableofcontents
\end{frame}
%=======
\section{Neural ODE: finish}
%=======
\begin{frame}{Neural ODE}
	\begin{block}{Adjoint functions}
		\vspace{-0.3cm}
		\[
			\ba_{\bz}(t) = \frac{\partial L(\by)}{\partial \bz(t)}; \quad \ba_{\btheta}(t) = \frac{\partial L(\by)}{\partial \btheta(t)}.
		\]
		\vspace{-0.6cm}
	\end{block}
	\begin{block}{Theorem (Pontryagin)}
	\vspace{-0.6cm}
	\[
	     \frac{d \ba_{\bz}(t)}{dt} = - \ba_{\bz}(t)^T \cdot \frac{\partial f(\bz(t), \btheta)}{\partial \bz}; \quad \frac{d \ba_{\btheta}(t)}{dt} = - \ba_{\bz}(t)^T \cdot \frac{\partial f(\bz(t), \btheta)}{\partial \btheta}.
	\]
	Do we know any initilal condition?
	\end{block}
	\begin{block}{Solution for adjoint function}
		\vspace{-0.8cm}
		\begin{align*}
			 \frac{\partial L}{\partial \btheta(t_0)} &= \ba_{\btheta}(t_0) =  - \int_{t_1}^{t_0} \ba_{\bz}(t)^T \frac{\partial f(\bz(t), \btheta)}{\partial \btheta(t)} dt + 0\\
			 \frac{\partial L}{\partial \bz(t_0)} &= \ba_{\bz}(t_0) =  - \int_{t_1}^{t_0} \ba_{\bz}(t)^T \frac{\partial f(\bz(t), \btheta)}{\partial \bz(t)} dt + \frac{\partial L}{\partial \bz(t_1)}\\
		\end{align*}
		\vspace{-1.5cm}
	\end{block}
	\textbf{Note:} These equations are solved back in time.
	\myfootnotewithlink{https://arxiv.org/abs/1806.07366}{Chen R. T. Q. et al. Neural Ordinary Differential Equations, 2018}   
\end{frame}
%=======
\begin{frame}{Neural ODE}
	\vspace{-0.2cm}
	\begin{block}{Forward pass}
		\vspace{-0.5cm}
		\[
			\bz(t_1) = \int^{t_1}_{t_0} f(\bz(t), \btheta) d t  + \bz_0 \quad \Rightarrow \quad \text{ODE Solver}
		\]
		\vspace{-0.6cm}
	\end{block}
	\begin{block}{Backward pass}
		\vspace{-0.8cm}
		\begin{equation*}
			\left.
				{\footnotesize 
				\begin{aligned}
					\frac{\partial L}{\partial \btheta(t_0)} &= \ba_{\btheta}(t_0) =  - \int_{t_1}^{t_0} \ba_{\bz}(t)^T \frac{\partial f(\bz(t), \btheta)}{\partial \btheta(t)} dt + 0 \\
					\frac{\partial L}{\partial \bz(t_0)} &= \ba_{\bz}(t_0) =  - \int_{t_1}^{t_0} \ba_{\bz}(t)^T \frac{\partial f(\bz(t), \btheta)}{\partial \bz(t)} dt + \frac{\partial L}{\partial \bz(t_1)} \\
					\bz(t_0) &= - \int^{t_0}_{t_1} f(\bz(t), \btheta) d t  + \bz_1.
				\end{aligned}
				}
			\right\rbrace
			 \Rightarrow
			\text{ODE Solver}
		\end{equation*}
		\vspace{-0.4cm} 
	\end{block}
	\textbf{Note:} These scary formulas are the standard backprop in the discrete case.
	\begin{figure}
		\centering
		\includegraphics[width=\linewidth]{figs/neural_ode}
	\end{figure}
	\myfootnotewithlink{https://arxiv.org/abs/1806.07366}{Chen R. T. Q. et al. Neural Ordinary Differential Equations, 2018}   
\end{frame}
%=======
\section{Continuous-in-time normalizing flows}
%=======
\begin{frame}{Continuous Normalizing Flows}
	\begin{block}{Discrete Normalizing Flows}
		\vspace{-0.8cm}
		  \[
		  \bz_{t+1} = f(\bz_t, \btheta); \quad \log p(\bz_{t+1}) = \log p(\bz_{t}) - \log \left| \det \frac{\partial f(\bz_t, \btheta)}{\partial \bz_{t}} \right| .
		  \]
		\vspace{-0.7cm}
	\end{block}
	\begin{block}{Continuous-in-time dynamic transformation}
		\vspace{-0.2cm}
		\[
			\frac{d\bz(t)}{dt} = f(\bz(t), \btheta).
		\]
		\vspace{-0.4cm}
	\end{block}
	Assume that function $f$ is uniformly Lipschitz continuous in $\bz$ and continuous in $t$. From Picardâ€™s existence theorem, it follows that the above ODE has a \textbf{unique solution}.
	\begin{block}{Forward and inverse transforms}
		\vspace{-0.7cm}
		\begin{align*}
			\bx &= \bz(t_1) = \bz(t_0) + \int_{t_0}^{t_1} f(\bz(t), \btheta) dt \\
			\bz &= \bz(t_0) = \bz(t_1) + \int_{t_1}^{t_0} f(\bz(t), \btheta) dt \\
		\end{align*}
	\end{block}
	\myfootnotewithlink{https://arxiv.org/abs/1912.02762}{Papamakarios G. et al. Normalizing flows for probabilistic modeling and inference, 2019}   
\end{frame}
%=======
\begin{frame}{Continuous Normalizing Flows}
	To train this flow we have to get the way to calculate the density~$p(\bz(t))$.
	\begin{block}{Theorem (special case of Kolmogorov-Fokker-Planck)}
		if function $f$ is uniformly Lipschitz continuous in $\bz$ and continuous in $t$, then
		\vspace{-0.3cm}
		\[
			\frac{d \log p(\bz(t))}{d t} = - \text{trace} \left( \frac{\partial f (\bz(t), \btheta)}{\partial \bz(t)} \right).
		\]
		\vspace{-0.5cm}
	\end{block}
	\textbf{Note:} Unlike discrete-in-time flows, the function $f$ does not need to be bijective, because uniqueness guarantees that the entire transformation is automatically bijective.
	\begin{block}{Density evaluation}
		\vspace{-0.4cm}
		\[
			\log p(\bx | \btheta) = \log p(\bz) - \int_{t_0}^{t_1} \text{trace}  \left( \frac{\partial f (\bz(t), \btheta)}{\partial \bz(t)} \right) dt.
		\]
		\textbf{Adjoint} method is used to integral evaluation.
	\end{block}
	\myfootnotewithlink{https://arxiv.org/abs/1806.07366}{Chen R. T. Q. et al. Neural Ordinary Differential Equations, 2018}   
\end{frame}
%=======
\begin{frame}{Continuous Normalizing Flows}
	\vspace{-0.6cm}
	\begin{block}{Forward transform + log-density}
		\vspace{-0.8cm}
		\[
			\begin{bmatrix}
				\bx \\
				\log p(\bx | \btheta)
			\end{bmatrix}
			= 
			\begin{bmatrix}
				\bz \\
				\log p(\bz)
			\end{bmatrix} + 
			\int_{t_0}^{t_1} 
			\begin{bmatrix}
				f(\bz(t), \btheta) \\
				- \text{trace} \left( \frac{\partial f(\bz(t), \btheta)}{\partial \bz(t)} \right) 
			\end{bmatrix} dt.
		\]
		\vspace{-0.6cm}
	\end{block}
	\begin{itemize}
		\item Discrete-in-time normalizing flows need invertible $f$. It costs $O(d^3)$ to get determinant of Jacobian.
		\item Continuous-in-time flows require only smoothness of $f$. It costs $O(d^2)$ to get trace of Jacobian.
	\end{itemize}
	\vspace{-0.5cm}
	\begin{minipage}[t]{0.4\columnwidth}
		\begin{figure}
			\centering
			\includegraphics[width=0.75\linewidth]{figs/cnf_flow.png}
		\end{figure}
	\end{minipage}%
	\begin{minipage}[t]{0.6\columnwidth}
		\begin{figure}
			  \centering
			  \includegraphics[width=0.8\linewidth]{figs/ffjord.png}
		\end{figure}
	\end{minipage}
	\myfootnotewithlink{https://arxiv.org/abs/1810.01367}{Grathwohl W. et al. FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models, 2018} 
\end{frame}
%=======
\section{Stochastic differential equations basics (SDE)}
%=======
\begin{frame}{Generative models zoo}
	\begin{tikzpicture}[
	 	basic/.style  = {draw, text width=2cm, drop shadow, rectangle},
	 	root/.style   = {basic, rounded corners=2pt, thin, text height=1.1em, text width=7em, align=center, fill=blue!40},
	 	level 1/.style={sibling distance=55mm},
	 	level 2/.style = {basic, rounded corners=6pt, thin, align=center, fill=blue!20, text height=1.1em, text width=9em, sibling distance=38mm},
	 	level 3/.style = {basic, rounded corners=6pt, thin,align=center, fill=blue!20, text width=8.5em},
	 	level 4/.style = {basic, thin, align=left, fill=pink!30, text width=7em},
	 	level 5/.style = {basic, thin, align=left, fill=pink!90, text width=7em},
		edge from parent/.style={->,draw},
		>=latex]
		
		% root of the the initial tree, level 1
		\node[root] {\Large Generative models}
		% The first level, as children of the initial tree
		child {node[level 2] (c1) {Likelihood-based}
			child {node[level 3] (c11) {Tractable density}}
			child {node[level 3] (c12) {Approximate density}}
		}
		child {node[level 2] (c2) {Implicit density}};
	
		% The second level, relatively positioned nodes
		\begin{scope}[every node/.style={level 4}]
			\node [below of = c11, yshift=-5pt, xshift=10pt] (c111) {Autoregressive models};
			\node [below of = c111, yshift=-5pt] (c112) {Normalizing Flows};
			\node [below of = c12, xshift=10pt] (c121) {VAEs};
			
			\node [below of = c2, xshift=10pt] (c21) {GANs};
		\end{scope}
				
		% The second level, relatively positioned nodes
		\begin{scope}[every node/.style={level 5}]
			\node [below of = c121] (c122) {\textbf{Diffusion \\ models}};
		\end{scope}
	
		% lines from each level 1 node to every one of its "children"
		\foreach \value in {1,2}
		\draw[->] (c11.194) |- (c11\value.west);
		
		\foreach \value in {1,2}
		\draw[->] (c12.194) |- (c12\value.west);
		
		\draw[->] (c2.194) |- (c21.west);
		
	\end{tikzpicture}
\end{frame}
%=======
\begin{frame}{Langevin dynamic}
	Imagine that we have some generative model $p(\bx | \btheta)$.
	\begin{block}{Statement}
		Let $\bx_0$ be a random vector. Then under mild regularity conditions for small enough $\eta$ samples from the following dynamics
		\[
			\bx_{t + 1} = \bx_t + \eta \frac{1}{2} \nabla_{\bx_t} \log p(\bx_t | \btheta) + \sqrt{\eta} \cdot \bepsilon, \quad \bepsilon \sim \cN(0, 1).
		\]
		will comes from $p(\bx | \btheta)$.
	\end{block}
	What do we get if $\bepsilon = \boldsymbol{0}$?
	\begin{block}{Energy-based model}
		\[
			p(\bx | \btheta) = \frac{\hat{p}(\bx | \btheta)}{Z_{\btheta}}, \quad \text{where } Z_{\theta} = \int \hat{p}(\bx | \btheta) d \bx
		\]
		\[
			\nabla_{\bx} \log p(\bx | \btheta) = \nabla_{\bx} \log \hat{p}(\bx | \btheta) - \nabla_{\bx} \log Z_{\btheta} = \nabla_{\bx} \log \hat{p}(\bx | \btheta)
		\]
	\end{block}
\end{frame}
%=======
\begin{frame}{Stochastic differential equation (SDE)}
	Let define stochastic process $\bx(t)$ with initial condition $bx(0) \sim p_0(\bx)$:
	\[
		d\bx = \mathbf{f}(\bx, t) dt + g(t) d \bw
	\]
	\begin{itemize}
	\item $\bw(t)$ is the standard Wiener process (Brownian motion)
	\[		
		\bw(t) - \bw(s) \sim \cN(0, t - s), \quad d \bw = \epsilon \cdot \sqrt{dt}, \, \text{where } \epsilon \sim \cN(0, 1).
	\]
	 \item $\mathbf{f}(\bx, t)$ is the \textbf{drift} function of $\bx(t)$.
	 \item $g(t)$ is the \textbf{diffusion} coefficient of $\bx(t)$.
	 \item If $g(t) = 0$ we get standard ODE.
	\end{itemize}
\end{frame}
%=======
\begin{frame}{Stochastic differential equation (SDE)}
	\[
		d\bx = \mathbf{f}(\bx, t) dt + g(t) d \bw
	\]
 	How to get distribution $p(\bx | t)$ for $\bx(t)$?
	\begin{block}{Theorem (Kolmogorov-Fokker-Planck)}
		Evolution of the distribution $p(\bx | t)$ is given by the folliwing ODE:
		\[
			\frac{\partial p(\bx|t)}{\partial t} = - \frac{\partial}{\partial \bx} \bigl( \mathbf{f}(\bx, t) p(\bx)\bigr) + \frac{1}{2} g^2(t) \frac{\partial^2 p(\bx | t)}{\partial \bx^2} 
		\]
	\end{block}
\end{frame}
%=======
\begin{frame}{Stochastic differential equation (SDE)}
	\begin{block}{Langevin SDE}
		Let consider special case of SDE with $g(t) = 1$ and $\mathbf{f}(\bx, t) = \frac{1}{2} \frac{\partial}{\partial \bx} \log p(\bx | t)$.
		\[
			d \bx = \frac{1}{2} \frac{\partial}{\partial \bx} \log p(\bx | t) d t + d \bw
		\]
		Let apply KFP theorem.
		\begin{multline*}
			\frac{\partial p(\bx|t)}{\partial t} =  - \frac{\partial}{\partial \bx}\left( p(\bx | t) \frac{1}{2} \frac{\partial}{\partial \bx} \log p(\bx | t) \right)  + \frac{1}{2} \frac{\partial^2 p(\bx | t)}{\partial \bx^2} = \\
			= - \frac{\partial}{\partial \bx}\left( \frac{1}{2} \frac{\partial}{\partial \bx} p(\bx | t) \right)  + \frac{1}{2} \frac{\partial^2 p(\bx | t)}{\partial \bx^2} = 0
		\end{multline*}
		The density is $p(\bx | t) = \text{const}$.
	\end{block}
\end{frame}
%=======
\begin{frame}{Stochastic differential equation (SDE)}
	\begin{block}{Langevin dynamic}
		Let discretize the Langevin SDE
		\[
			\bx_{t + 1} = \bx_t + \eta \frac{1}{2} \frac{\partial}{\partial \bx} \log p(\bx | t) + \sqrt{\eta} \cdot \bepsilon, \quad \bepsilon \sim \cN(0, 1).
		\]
	\end{block}
\end{frame}
%=======
\begin{frame}{Summary}
	\begin{itemize}
		\item Adjoint method generalizes backpropagation procedure and allows to train Neural ODE solving ODE for adjoint function back in time.
		\vfill
		\item Fokker-Planck theorem allows to construct continuous-in-time normalizing flow with less functional restrictions.
		\vfill
		\item FFJORD model makes such kind of flows scalable.
		\vfill
	\end{itemize}
\end{frame}
\end{document} 